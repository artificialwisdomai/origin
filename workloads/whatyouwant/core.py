import enum
import heapq
import os
import time
from typing import Optional

import faiss
import retro_pytorch
import transformers

from fastapi import FastAPI
from pydantic import BaseModel

# The hardcoded path to docs.
DOCS_PATH = os.environ["DOCS_PATH"]

def extractParagraphsHackily(path, name):
    "Hackily extract paragraphs from Markdown."

    paras = []
    lines = []
    with open(os.path.join(path, name)) as handle:
        for line in handle.read().split("\n"):
            # Markdown headers, ReST headers
            if line.startswith(("#", "=")):
                para = " ".join(lines).strip()
                if para: paras.append(para)
                lines = []
            else: lines.append(line)
    return paras

ctxs = {name: extractParagraphsHackily(DOCS_PATH, name)
        for name in os.listdir(DOCS_PATH)}
print("Loaded:", len(ctxs), "documents,",
      sum(len(p) for p in ctxs.values()), "paragraphs")

def makeHF():
    return transformers.pipeline("question-answering",
    model="deepset/tinyroberta-squad2")

def makeRETRO():
    return retro_pytorch.RETRO(
        chunk_size=64,
        max_seq_len=2048,
        enc_dim=896,
        enc_depth=2,
        dec_dim=796,
        dec_depth=12,
        dec_cross_attn_layers=(3, 6, 9, 12),
        heads=8,
        dim_head=64,
        dec_attn_dropout=0.25,
        dec_ff_dropout=0.25,
        use_deepnet=True,
    )

# retro = makeRETRO()
# print("Instantiated RETRO", retro)
pipeline = makeHF()
print("Instantiated HF pipeline", pipeline)

# This is the Flask pattern for FastAPI. First, we declare an application.
app = FastAPI()

# Then, we declare some models. We are mapping between JSON values outside and
# Python values inside.

class LanguageModel(enum.Enum):
    "A language model."
    BERT = "bert"

class Task(enum.Enum):
    "An inference task."
    qa = "qa-extractive"

class InferenceRequest(BaseModel):
    "A request to perform an inference task."
    model :LanguageModel
    task :Task
    query :str

class InferredText(BaseModel):
    "A text object generated by a language model."
    score :Optional[float]
    location :str
    result :str

class InferenceResponse(BaseModel):
    "A response from an inference task."
    duration :float
    answers :list[InferredText]

# Finally, we declare some routes. Each route implements some behavior by
# taking some modeled objects as input and returning some modeled objects as
# output, with some bells and whistles.

@app.get("/v0/models")
async def listModels() -> list[LanguageModel]:
    "Enumerate the available language models."
    return list(LanguageModel)

@app.post("/v0/infer")
async def infer(req :InferenceRequest) -> InferenceResponse:
    "Perform inference with a configured model over a data source."
    # Number of results to return.
    k = 5
    best = []
    # XXX gonna want a context manager for this
    start = time.perf_counter()
    # Open-coded variant of heapq.nlargest avoiding itertools.chain...
    for name, paras in ctxs.items():
        results = pipeline(question=[req.query] * len(paras), context=paras,
                           top_k=k)
        # ...because HF only returns a list here if there's more than one
        # result, and otherwise it is bare. And also only returns a list of
        # list if there was more than one result in more than one context.
        if not isinstance(results, list): results = [results]
        for l in results:
            if not isinstance(l, list): l = [l]
            for p in l:
                item = p["score"], name, p["answer"]
                if len(best) < k: heapq.heappush(best, item)
                else: heapq.heappushpop(best, item)
    # Users will want best results first.
    best.sort(reverse=True)
    duration = time.perf_counter() - start
    return InferenceResponse(
            duration=duration,
            answers=[InferredText(score=score, location=name, result=result)
                     for (score, name, result) in best])
